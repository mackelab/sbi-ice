{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF,ConstantKernel,Matern\n",
    "import pickle\n",
    "from sbi.utils import get_density_thresholder, RestrictedPrior, BoxUniform,process_prior\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "\n",
    "from sbi_ice.utils import plotting_utils,custom_priors,plotting_utils,misc\n",
    "from sbi_ice.simulators import Layer_Tracing_Sim as lts\n",
    "import sbi_ice.utils.noise_model as noise_model\n",
    "data_dir,output_dir = misc.get_data_output_dirs()\n",
    "work_folder = misc.get_project_root()\n",
    "color_opts = plotting_utils.setup_plots()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we set up the geometry object we will use and define which setup we want for the geometry/velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = \"Ekstrom\"\n",
    "# data_file = Path(data_dir,name,\"setup_files\",\"flowtube_full.csv\")\n",
    "\n",
    "\n",
    "name = \"Synthetic_long\"\n",
    "data_file = Path(data_dir,name,\"setup_files\",\"flowtube_tmb_final.csv\")\n",
    "\n",
    "df = pd.read_csv(data_file)\n",
    "xmb = df[\"x_coord\"].to_numpy() #x - coordinates of domain\n",
    "tmb = df[\"tmb\"].to_numpy() #total mass balance\n",
    "Lx = xmb.max() - xmb.min()\n",
    "\n",
    "nx_iso             = 500 # Number of points in the x-direction\n",
    "ny_iso             = 1 # Number of points in the y-direction\n",
    "dt                 = 0.5 # [yr] timestep for advection scheme\n",
    "\n",
    "\n",
    "geom = lts.Geom(nx_iso=nx_iso,ny_iso=ny_iso) #This will be the main object we interact with\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we set up the smb and bmb fields. It is of course possible to just set, e.g. \"smb = 0.3\" everywhere.\n",
    "However, here we define a prior distribution over possible smb fields and then sample form that distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123456) #Set the seed for reproducibility\n",
    "\n",
    "#We use a perturbed GP prior. First, we have a regular GP prior with mean 0, variance 1, and a specified lengthscale (with a Matern kernel).\n",
    "#We then perturb the GP samples by multiplying them by a variance factor and adding a constant mean, which are independently sampled from specified distributions.\n",
    "\n",
    "\n",
    "GP_mean_mu= 0.5 #If GP mean sampled from a Gaussian, this is the mean of that Gaussian\n",
    "GP_mean_sd= 0.25 #If GP mean sampled from a Gaussian, this is the standard deviation of that Gaussian\n",
    "# GP_mean_min = 0.0 #If GP mean sampled from a uniform, this is the minimum of that uniform\n",
    "# GP_mean_max = 1.0 #If GP mean sampled from a uniform, this is the maximum of that uniform\n",
    "GP_var_min= 0.1 #Minimum variance of GP\n",
    "GP_var_max= 0.3 #Maximum variance of GP\n",
    "length_scale= 2.5E+3 #Length scale of GP\n",
    "nu= 2.5 #Smoothness of GP\n",
    "\n",
    "ker =  Matern(length_scale=length_scale,nu=nu) #Matern kernel as Gaussian is too smooth\n",
    "gpr = GaussianProcessRegressor(kernel=ker)\n",
    "mvn_mean,mvn_cov = gpr.predict(xmb.reshape(-1,1),return_cov=True) #We take the usual GP marginalized covariance on our grid, and make sure it is positive definite by adding a small diagonal matrix\n",
    "eps = 1e-6\n",
    "a = np.zeros(shape = mvn_cov.shape)\n",
    "np.fill_diagonal(a,eps)\n",
    "mvn_cov += a\n",
    "mvn_mean = torch.from_numpy(mvn_mean)\n",
    "mvn_cov = torch.from_numpy(mvn_cov)\n",
    "custom_prior = custom_priors.AppendedMVN(torch.tensor([GP_mean_mu]),torch.tensor([GP_mean_sd]),torch.tensor([GP_var_min]),\n",
    "                                            torch.tensor([GP_var_max]),mvn_mean,mvn_cov)\n",
    "\n",
    "# This is a function from the sbi utils, making it easier to sample from the prior and enforces strict bounds on the prior.\n",
    "prior, *_ = process_prior(custom_prior,\n",
    "                            custom_prior_wrapper_kwargs=dict(lower_bound=torch.cat((torch.tensor([GP_mean_mu-3*GP_mean_sd,GP_var_min]),-5.0*torch.ones(mvn_mean.size()))), \n",
    "                                                            upper_bound=torch.cat((torch.tensor([GP_mean_mu+3*GP_mean_sd,GP_var_max]),5.0*torch.ones(mvn_mean.size())))))\n",
    "\n",
    "\n",
    "# prior, *_ = process_prior(custom_prior,\n",
    "#                             custom_prior_wrapper_kwargs=dict(lower_bound=torch.cat((torch.tensor([GP_mean_min,GP_var_min]),-2.0*torch.ones(mvn_mean.size()))), \n",
    "#                                                             upper_bound=torch.cat((torch.tensor([GP_mean_max,GP_var_max]),2.0*torch.ones(mvn_mean.size())))))\n",
    "\n",
    "#We sample a lot of smbs from this prior for plotting purposes, but only take one to use in the simulation.\n",
    "samples = custom_prior.sample((1,)).numpy()\n",
    "bmb_list = []\n",
    "smb_cnst_mean_array = samples[:,0]\n",
    "smb_sd_array= samples[:,1]\n",
    "smb_unperturbed_array = samples[:,2:]\n",
    "smb_samples = np.expand_dims(smb_cnst_mean_array,1) + np.expand_dims(smb_sd_array,1)*smb_unperturbed_array\n",
    "\n",
    "smb_cnst_mean = smb_cnst_mean_array[0].copy()\n",
    "smb_sd = smb_sd_array[0].copy()\n",
    "smb_unperturbed = smb_unperturbed_array[0].copy()\n",
    "smb = smb_samples[0].copy()\n",
    "\n",
    "#Plot the smb samples\n",
    "fig,ax = plt.subplots(1,1,figsize = (4,2))\n",
    "ax.plot(xmb,smb_samples[:20].T,color = plotting_utils.color_opts[\"colors\"][\"prior\"],alpha = 0.3)\n",
    "print(smb_cnst_mean,smb_sd)\n",
    "#ax.plot(xmb,smb)\n",
    "#Finally, set the smb and bmb fields of the geom object.\n",
    "smb_regrid,bmb_regrid = lts.init_geom_from_fname(geom,data_file,smb=smb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"smb_cnst_mean: \",smb_cnst_mean)\n",
    "print(\"smb_sd: \",smb_sd)\n",
    "print(\"smb_unperturbed: \",smb_unperturbed)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define the timestepping of the layer tracing scheme. In general, we want the near surface layers at a higher resolution, so what I tend to do is to simulate in 2 phases. The first is quite coarse in terms of vertical resolution, and the second is finer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_surface_phase1 = 25\n",
    "n_base_phase1 = 100\n",
    "time_phase1 = 500\n",
    "n_surface_phase2 = 2\n",
    "n_base_phase2 = 20\n",
    "time_phase2 = 500\n",
    "\n",
    "sched1 = lts.Scheduele(time_phase1,dt,n_surface_phase1,n_base_phase1)\n",
    "sched2 = lts.Scheduele(time_phase2,dt,n_surface_phase2,n_base_phase2)\n",
    "print(sched1.total_iterations)\n",
    "print(sched2.total_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now simulate\n",
    "\n",
    "geom.initialize_layers(sched1,10)\n",
    "lts.sim(geom,smb_regrid,bmb_regrid,sched1)\n",
    "#Can save intermediate results here if we want\n",
    "geom.initialize_layers(sched2,0)\n",
    "lts.sim(geom,smb_regrid,bmb_regrid,sched2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot select layers\n",
    "plot_indices = [-50,-100,-200,-300]\n",
    "print(geom.age_iso[plot_indices])\n",
    "fig,axs = plotting_utils.plot_isochrones_1d(geom.x,geom.bs.flatten(),geom.ss.flatten(),geom.dsum_iso[:,0,plot_indices],geom.age_iso[plot_indices],bmb_regrid,smb_regrid,real_layers=None,trackers=geom.extract_active_trackers())\n",
    "axs[1].set_ylim(-300,100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, we are done now. However, when doing inference, it is important to capture the observational noise in our layers. The code below does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sbi_ice.loaders import ShortProfileLoader\n",
    "from pathlib import Path\n",
    "from scipy.fft import rfftfreq\n",
    "\n",
    "\n",
    "\n",
    "# loader = ShortProfileLoader.ShortProfileLoader(Path(data_dir,\"Ekstrom\",\"setup_files\"),Path(output_dir,\"Ekstrom\"),\"exp2\",gt_version=\"v0\")\n",
    "# PSD_dict = pickle.load(Path(output_dir,\"Ekstrom\",\"exp2\",\"PSD_matched_noise.p\").open(\"rb\"))\n",
    "# #print(PSD_dict)\n",
    "# freq = PSD_dict[\"freqs\"][0]\n",
    "# PSD_log_mean = PSD_dict[\"PSD_log_diffs_means\"][0]\n",
    "# PSD_log_var = PSD_dict[\"PSD_log_diffs_vars\"][0]\n",
    "\n",
    "freq = torch.from_numpy(rfftfreq(geom.x.size,d=geom.dx))\n",
    "print(freq)\n",
    "PSD_log_mean = -8.0*(1-torch.exp(-freq/(5e-4)))\n",
    "PSD_log_var = 0.5*torch.ones_like(freq)\n",
    "PSD_dict = {\"freqs\":[freq],\"PSD_log_diffs_means\":[PSD_log_mean],\"PSD_log_diffs_vars\":[PSD_log_var]}\n",
    "\n",
    "\n",
    "#The function that adds noise takes positions of the layers in (x,depth) rather than (x,elevation), so we first convert the layer elevations to depths.\n",
    "layers = torch.from_numpy(geom.dsum_iso[:,0,:].copy()).T\n",
    "\n",
    "\n",
    "layers_thickness = geom.dsum_iso[:,0,:].T\n",
    "heights = layers_thickness + geom.bs.flatten()\n",
    "layer_depths = geom.ss.flatten() - heights\n",
    "layer_depths = torch.from_numpy(layer_depths)\n",
    "layer_depths = torch.flip(layer_depths,dims=(0,))\n",
    "\n",
    "#Repeat the x array to have the same shape as the layers\n",
    "layer_xs = torch.stack([torch.Tensor(geom.x) for i in range(layer_depths.shape[0])])\n",
    "\n",
    "#Add noise to the layers. Here we return the base error (function of x), depth correction error (function of z), picking error (from labelling the isochrones), and total error (function of x and z)\n",
    "#base_error,depth_corr,picking_error,error = modelling_utils.depth_error(layer_xs,layer_depths)\n",
    "base_error,depth_corr,picking_error,error = noise_model.depth_error(layer_xs,layer_depths,freq,PSD_log_mean,PSD_log_var)\n",
    "\n",
    "#We flip the error vertically to match with the layer elevations again\n",
    "flipped_error = torch.flip(error,dims=(0,))\n",
    "layers += flipped_error\n",
    "\n",
    "#Plot the noisy layers\n",
    "plt.plot(geom.x,geom.ss.flatten(),\"k\")\n",
    "plt.plot(geom.x,geom.bs.flatten(),\"k\")\n",
    "for j in plot_indices:\n",
    "    plt.plot(geom.x,geom.bs.flatten() + layers[j,:].numpy(),label='Layer {}'.format(j),color=\"red\")\n",
    "    plt.plot(geom.x,geom.bs.flatten() + geom.dsum_iso[:,0,j],color=\"blue\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for layer in loader.real_layers:\n",
    "#     plt.plot(geom.x,layer,color=\"green\")\n",
    "plt.ylim(-100,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shelf = \"Synthetic_long\"\n",
    "output_file = Path(output_dir,shelf,\"exp3\",\"real_layers.csv\") #Save the real layer elevations\n",
    "supp_file =  Path(output_dir,shelf,\"exp3\",\"real_layers_supp.npy\") #Save the indices of the layers we save\n",
    "mb_file =  Path(output_dir,shelf,\"exp3\",\"real_mb.p\") #Save the smb and bmb fields\n",
    "noise_dist_file =  Path(output_dir,shelf,\"exp3\",\"PSD_matched_noise.p\") #Save the noise distribution\n",
    "\n",
    "plot_indices = np.array(plot_indices,dtype=int)\n",
    "df = pd.DataFrame(geom.x,columns = [\"x_coord\"])\n",
    "for i in range(len(plot_indices)):\n",
    "    df[\"layer {}\".format(i+1)] = geom.ss.flatten() - (layers.numpy().T[:,plot_indices[i]]+geom.bs.flatten()) #We save layer elevations above sea level instead of cumulative thicknesses\n",
    "\n",
    "df.to_csv(output_file)\n",
    "true_df = pd.read_csv(output_file)\n",
    "np.save(supp_file,plot_indices)\n",
    "\n",
    "mb_df = pd.DataFrame(np.vstack((geom.x,smb_regrid.flatten(),bmb_regrid.flatten())).T,columns = [\"x_coord\",\"smb\",\"bmb\"])\n",
    "with open(mb_file,\"wb\") as f:\n",
    "    pickle.dump(dict(x_coord=xmb,smb_const_mean=smb_cnst_mean,smb_var=smb_sd,smb_unperturbed=smb_unperturbed,bmb_regrid=bmb_regrid.flatten()),f)\n",
    "with open(mb_file,\"rb\") as f:\n",
    "    out = pickle.load(f)\n",
    "\n",
    "with open(noise_dist_file,\"wb\") as f:\n",
    "    pickle.dump(PSD_dict,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firedrake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
